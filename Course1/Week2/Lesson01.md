# ğŸ“˜ Multiple Linear Regression â€“ Detailed Notes

This lecture explains **Multiple Linear Regression**, which allows us to use **multiple features (inputs)** to make better predictions compared to using only one feature.  
It also introduces **notations** ğŸ“‘ and a **compact vectorized form** âš¡.

---

## 1ï¸âƒ£ From Single Feature to Multiple Features

### ğŸ”¹ Single Feature Model (Univariate Regression)
$$
f_{w,b}(x) = wx + b
$$

- Uses only **one input feature**.  
- Example: Predicting house price using only **size of the house (sq. feet)**.

![](./images(w2)/onef.png)

---

### ğŸ”¹ Multiple Features Model (Multiple Linear Regression)
- Uses **several input features** â†’ more accurate predictions.  
- Example: House price prediction using:  
  1. ğŸ  Size of the house ($x_1$)  
  2. ğŸ›ï¸ Number of bedrooms ($x_2$)  
  3. ğŸ¢ Number of floors ($x_3$)  
  4. ğŸ“… Age of the home in years ($x_4$)  



---

## 2ï¸âƒ£ Notation for Multiple Features âœï¸

To handle multiple inputs, we use structured notation:

| Symbol | Meaning | Example (if n=4) |
|--------|---------|------------------|
| **n** | Number of features | n = 4 |
| **x_j** | The j-th feature | $x_1$ = size, $x_4$ = age |
| **x (vector)** | All features as a vector | [ $x_1, x_2, x_3, x_4$ ] |
| **x^(i)** | i-th training example (vector) | $x^{(2)} = [1416, 3, 2, 40]$ |
| **x^(i)_j** | j-th feature of i-th example | $x^{(2)}_3 = 2$ |

![](./images(w2)/mf.png)

---

## 3ï¸âƒ£ The Multiple Linear Regression Model ğŸ“Š

The model learns **parameters (weights)** for each feature + a bias term.

---

### ğŸ”¹ Expanded Form
$$
f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b
$$

For **n** features:
$$
f_{w,b}(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b
$$

---

### ğŸ”¹ Example with House Prices
$$
f_{w,b}(x) = 0.1x_1 + 4x_2 + 10x_3 - 2x_4 + 80
$$

- ğŸ“ **0.1** â†’ Each extra square foot adds **$100**  
- ğŸ›ï¸ **4** â†’ Each bedroom adds **$4000**  
- ğŸ¢ **10** â†’ Each floor adds **$10,000**  
- ğŸ“‰ **-2** â†’ Each year reduces price by **$2000**  
- ğŸ’µ **80** â†’ Base price = **$80,000**  

![](./images(w2)/prev.png)

---

## 4ï¸âƒ£ Vectorized Notation (Compact Form) âš¡

Instead of writing many terms, we use the **dot product**:

$$
f_{w,b}(x) = w \cdot x + b
$$

- **w** = [ $w_1, w_2, ..., w_n$ ] â†’ parameters (weights)  
- **x** = [ $x_1, x_2, ..., x_n$ ] â†’ features  

### ğŸ”¹ Dot Product Definition:
$$
w \cdot x = w_1x_1 + w_2x_2 + ... + w_nx_n
$$

âœ” Compact form = **shorter, cleaner, faster** to compute.  

![](./images(w2)/dot.png)

---

## 5ï¸âƒ£ Important Terminology ğŸ“–

- **Multiple Linear Regression** â†’ Linear regression with **multiple input features**.  
- **Univariate Regression** â†’ Linear regression with **one feature only**.  
- **Multivariate Regression** â†’ Different concept (predicting multiple outputs), not covered here.  

---

## 6ï¸âƒ£ Key Insights ğŸŒŸ

âœ… Adding more features improves prediction accuracy.  
âœ… Each weight ($w_j$) shows how much that feature affects the result.  
âœ… Positive weight â†’ increases output, Negative weight â†’ decreases output.  
âœ… Vectorization makes the model **easier** and **faster** to implement in code.  

---

## ğŸš€ Next Step
Learn about **vectorization techniques** to compute predictions for **many examples at once** efficiently.

---

# ğŸ“˜ Multiple Linear Regression  

This lecture explains **Multiple Linear Regression**, which allows us to use **multiple features (inputs)** to make better predictions compared to using only one feature.  
It also introduces **notations** ğŸ“‘ and a **compact vectorized form** âš¡.

---

## 1ï¸âƒ£ From Single Feature to Multiple Features

### ğŸ”¹ Single Feature Model (Univariate Regression)
\[
f_{w,b}(x) = wx + b
\]

- Uses only **one input feature**.  
- Example: Predicting house price using only **size of the house (sq. feet)**.

ğŸ“Œ *Insert Screenshot Here â¡ï¸ (Single feature table)*

---

### ğŸ”¹ Multiple Features Model (Multiple Linear Regression)
- Uses **several input features** â†’ more accurate predictions.  
- Example: House price prediction using:  
  1. ğŸ  Size of the house (\(x_1\))  
  2. ğŸ›ï¸ Number of bedrooms (\(x_2\))  
  3. ğŸ¢ Number of floors (\(x_3\))  
  4. ğŸ“… Age of the home in years (\(x_4\))  

ğŸ“Œ *Insert Screenshot Here â¡ï¸ (Multiple features table)*

---

## 2ï¸âƒ£ Notation for Multiple Features âœï¸

To handle multiple inputs, we use structured notation:

| Symbol | Meaning | Example (if \(n=4\)) |
|--------|---------|-----------------------|
| \(n\) | Number of features | \(n=4\) |
| \(x_j\) | The \(j^{th}\) feature | \(x_1\) = size, \(x_4\) = age |
| \(\vec{x}\) | Vector of all features | \([x_1, x_2, x_3, x_4]\) |
| \(x^{(i)}\) | The \(i^{th}\) training example (a vector) | \(x^{(2)} = [1416, 3, 2, 40]\) |
| \(x^{(i)}_j\) | Value of the \(j^{th}\) feature in the \(i^{th}\) example | \(x^{(2)}_3 = 2\) |

ğŸ“Œ *Insert Screenshot Here â¡ï¸ (Notation with training examples)*

---

## 3ï¸âƒ£ The Multiple Linear Regression Model ğŸ§®

The model learns **parameters (weights)** for each feature + a bias term.

### ğŸ”¹ Expanded Form
\[
f_{\vec{w}, b}(\vec{x}) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b
\]

For \(n\) features:
\[
f_{\vec{w}, b}(\vec{x}) = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
\]

---

### ğŸ”¹ Example with House Prices
\[
f_{\vec{w}, b}(\vec{x}) = 0.1x_1 + 4x_2 + 10x_3 - 2x_4 + 80
\]

- ğŸ“ **0.1** â†’ Each extra square foot adds **$100**  
- ğŸ›ï¸ **4** â†’ Each bedroom adds **$4000**  
- ğŸ¢ **10** â†’ Each floor adds **$10,000**  
- ğŸ“‰ **-2** â†’ Each year reduces price by **$2000**  
- ğŸ’µ **80** â†’ Base price = **$80,000**  

ğŸ“Œ *Insert Screenshot Here â¡ï¸ (Model with example equation)*

---

## 4ï¸âƒ£ Vectorized Notation (Compact Form) âš¡

Instead of writing many terms, we use the **dot product**:

\[
f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b
\]

- \(\vec{w} = [w_1, w_2, \dots, w_n]\) â†’ parameters (weights)  
- \(\vec{x} = [x_1, x_2, \dots, x_n]\) â†’ features  

### ğŸ”¹ Dot Product Definition:
\[
\vec{w} \cdot \vec{x} = w_1x_1 + w_2x_2 + \dots + w_nx_n
\]

âœ” Compact form = **shorter, cleaner, faster** to compute.  

ğŸ“Œ *Insert Screenshot Here â¡ï¸ (Dot product vector form)*

---

## 5ï¸âƒ£ Important Terminology ğŸ“–

- **Multiple Linear Regression** â†’ Linear regression with **multiple input features**.  
- **Univariate Regression** â†’ Linear regression with **one feature only**.  
- **Multivariate Regression** â†’ Different concept (predicting multiple outputs), not covered here.  

---

## 6ï¸âƒ£ Key Insights ğŸŒŸ

âœ… Adding more features improves prediction accuracy.  
âœ… Each weight (\(w_j\)) shows how much that feature affects the result.  
âœ… Positive weight â†’ increases output, Negative weight â†’ decreases output.  
âœ… Vectorization makes the model **easier** and **faster** to implement in code.  

---

## ğŸš€ Next Step
Learn about **vectorization techniques** to compute predictions for **many examples at once** efficiently.

---
